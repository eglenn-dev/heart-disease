---
title: "Heart Disease Data Analysis"
subtitle: "heart disease analysis and building a prediction model"
author: "Ethan Glenn"
date: "2025-04-08"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false    
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

## Background

Heart disease remains a leading cause of morbidity and mortality worldwide. Early identification of individuals at high risk is crucial for timely intervention and prevention. This project is aimed to leverage machine learning to develop a predictive model for assessing the likelihood of heart disease based on patient clinical data. This data utilized was sources from the widely recognized UC Irvine Machine Learning Repository dataset. The objective of this is to build a reliable model that could potentially aid healthcare professionals in risk stratification. 

## Exploring the Data

First, let's take a looks at the data features, and the first few columns that we will be working with.

```{python}
#| echo: true
df = pd.read_csv('heart-disease.csv')
```

```{python}
df.head()
```

```{python}
df.info()
```

Now I want to take a look at the distribution of our target, the num column. This column indicates the present of heart disease on a scale of 0-4. `0` being no presence, and `1-4` indicating beginning to severe heart disease. 

```{python}
df["num"].value_counts()
```

## Data Cleaning

Before we get to training a model, we will make sure that the number of missing values is limited, and make sure we handel data outliers correctly. 

First we will check a few things, such as number of unique data entries, as well as the feature data types. 

```{python}
df.nunique()
```

```{python}
df.dtypes
```

We can see that the `sex` column is unique, with only two unique options. 

```{python}
df['sex'].unique()
```

The chest pain column is also as expected with no missing values. It should be a scale of 1-4 indicating severity of chest pain in the patient. 

```{python}
df['cp'].unique()
```

We can see that there are missing values from the `thal` column. Thalassemia (`thal`) is a test of blood flow through the heart during a stress test of some kind of physical activity in the patient. We are going to have to fix that as this would be a good indicator of heart disease. 

```{python}
df['thal'].unique()
```

In fact, it is the only column with `null` values present.

```{python}
df.isnull().sum()
```

To fix this, we will impute the number based on the mean of the other data in the `thal` column. This will allow us to use the other data in the column, without having to remove it becuase it is incomplete. 

```{python}
df_imputed = df.fillna(df.mean())
```

After imputing the values, there are now no `null` values present in the dataset. 

```{python}
df_imputed.isnull().sum()
```

We will also check for duplicate rows. We can see that there are none.

```{python}
duplicated = df_imputed.duplicated().sum()
print(f'There are {duplicated} duplicated rows')
```

### Creating the Target

Because we are trying to predict whether someone is prone to having heart disease, we are going to make a new column. If there is no heart disease, the value will stay 0, but if there is any other presence (1-4) it will indicate that with a one. Either there is a presence, or there isn't.

```{python}
df_imputed['disease_binary'] = (df_imputed['num'] > 0).astype(int)
```

### Handling Outliers

We know there is going to be some presence of outliers, so we are doing to implement a Interquartile Range method to identify and remove outliers that will effect our models performance. 

```{python}
continous_features = ['age','trestbps','chol','thalach','oldpeak']
def outliers(df_out, drop = False):
    for each_feature in df_out.columns:
        feature_data = df_out[each_feature]
        Q1 = np.percentile(feature_data, 25.)
        Q3 = np.percentile(feature_data, 75.)
        IQR = Q3-Q1
        outlier_step = IQR * 1.5
        outliers = feature_data[~((feature_data >= Q1 - outlier_step) & (feature_data <= Q3 + outlier_step))].index.tolist()
        if not drop:
            print('For the feature {}, No of Outliers is {}'.format(each_feature, len(outliers)))
        if drop:
            df.drop(outliers, inplace = True, errors = 'ignore')
            print('Outliers from {} feature removed'.format(each_feature))

outliers(df_imputed[continous_features])

outliers(df_imputed[continous_features], drop=True)

df_cleaned = df_imputed.copy()
```

## Data Exploration

In this section a series of charts will be generated and commented on in regards to their validity and what they tell us about the data. 

In this we are able to see that while no exactly split, the distribution between diseased and not diseased is even enough to get accurate results. 

```{python}
df_cleaned['disease'] = df_cleaned['num'].apply(lambda x: 'Yes' if x != 0 else 'No')
percent_diseased = df_cleaned['disease'].value_counts(normalize=True) * 100

plt.figure(figsize=(8, 6))
sns.countplot(x='disease', data=df_cleaned)
plt.title('Distribution of Disease')
plt.xlabel('Disease')
plt.ylabel('Count')
plt.show()

percent_diseased
```

One factor of note, as shown in the following chart, is that there are far more men with heart disease, than women. The distribution of males and females is relatively even, with a slight lean towards males. 

```{python}
sex_df = df_cleaned.copy()
sex_df['sex'] = sex_df['sex'].apply(lambda x: 'Female' if x == 0 else 'Male')

plt.figure(figsize=(8, 6))
sns.countplot(x='sex', hue='disease', data=sex_df)
plt.title('Sex Distribution by Disease')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.legend(title='Disease')
plt.show()
```

The following chart highlights that while various types of chest pain occur, the absence of typical/atypical/non-anginal pain (classified as Type 4) is paradoxically the strongest indicator among these categories for the presence of heart disease within this specific dataset. Conversely, reporting non-anginal pain (Type 3) is most common among those without heart disease.

- Chest Pain Type 1 (Typical Angina)
- Chest Pain Type 2 (Atypical Angina)
- Chest Pain Type 3 (Non-anginal Pain)
- Chest Pain Type 4 (Asymptomatic)

```{python}
plt.figure(figsize=(8, 6))
sns.countplot(x='cp', hue='disease', data=df_cleaned)
plt.title('Chest Pain Type Distribution by Disease')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Disease')
plt.show()
```

As would be expected, we can see that there are normal distributions on the rest of our data feautes, as demonstrated below. 

```{python}
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 8))
axes = axes.flatten()

for i, feature in enumerate(continous_features):
    sns.histplot(df_cleaned[feature], kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {feature}')

for i in range(len(continous_features), len(axes)):
  axes[i].axis('off')

plt.tight_layout()
plt.show()
```

The following is a correlation confusion matrix between all the features. 

```{python}
sns.set(style="white")
plt.rcParams['figure.figsize'] = (15, 10)
numerical_df = df_cleaned.select_dtypes(include=np.number)
sns.heatmap(numerical_df.corr(), annot = True, linewidths=.5, cmap="Blues")
plt.title('Corelation Between Variables', fontsize = 30)
plt.show()
```

## Model Generation

```{python}
#| echo: false
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score
from sklearn.preprocessing import StandardScaler
```

### Preparing Features and Target

Separating the dataset into features (X) and the target variable (y). The target variable is the binary column indicating the presence of heart disease. This step ensures the model focuses only on relevant predictors.

```{python}
feature_cols = [col for col in df_cleaned.columns if col not in ['num', 'disease_binary', 'disease', 'disease_label']]
X = df_cleaned[feature_cols]
y = df_cleaned['disease_binary']

print(f"\n--- Preparing for Modeling ---")
print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Target distribution:\n{y.value_counts()}")
```

### Splitting Data

Splitting the data into training (80%) and testing (20%) sets. Stratification ensures the target variable's distribution is consistent across both sets, which is important for balanced evaluation.

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}")
```

### Scaling Numeric Columns

Identifying numeric columns that need scaling to standardize their ranges. Categorical numeric columns (e.g., sex, cp) are excluded since they represent discrete categories, not continuous values.

```{python}
numeric_cols_to_scale = X_train.select_dtypes(include=np.number).columns.tolist()
known_categorical_numeric = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
numeric_cols_final = [col for col in numeric_cols_to_scale if col not in known_categorical_numeric]
numeric_cols_final = [col for col in numeric_cols_final if col in X_train.columns]

print(f"\nNumeric columns identified for scaling: {numeric_cols_final}")
```

Standardizing numeric features to have a mean of 0 and a standard deviation of 1. This helps Gradient Boosting models perform better by ensuring all features are on the same scale.

```{python}
scaler = StandardScaler()

X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numeric_cols_final] = scaler.fit_transform(X_train[numeric_cols_final])
X_test_scaled[numeric_cols_final] = scaler.transform(X_test[numeric_cols_final])

print("Features scaled.")
print("Scaled Train Data Head:\n", X_train_scaled.head())
```

### Training the Model

Training a Gradient Boosting Classifier with 100 estimators, a learning rate of 0.1, and a maximum tree depth of 3. Gradient Boosting combines multiple weak learners (decision trees) to create a strong predictive model.

```{python}
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train_scaled, y_train)
print("Model training complete.")
```

Generating predictions for both the training and testing datasets. These predictions will be used to evaluate the model's performance.

```{python}
y_pred_train = gb_model.predict(X_train_scaled)
y_pred_test = gb_model.predict(X_test_scaled)
```

Evaluating the model's performance using metrics like accuracy, recall, and precision. The classification report provides a detailed breakdown of the model's performance for each class (disease vs. no disease).

```{python}
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")

print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_pred_test, target_names=['No Disease (0)', 'Disease (1)']))
```

Plotting the confusion matrix as a heatmap for easier interpretation of the model's performance. This visualization highlights the balance between true positives, true negatives, false positives, and false negatives. 

```{python}
cm = confusion_matrix(y_test, y_pred_test)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease (0)', 'Disease (1)'], yticklabels=['No Disease (0)', 'Disease (1)'])
plt.title('Confusion Matrix (Test Set)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()
```

### Hyper Parameter Tuning

Setting up a grid of hyperparameter for the Gradient Boosting model. The grid includes different values for the number of estimators, learning rate, maximum tree depth, and subsample ratio. GridSearchCV will test all combinations of these parameters using 5-fold cross-validation to find the best configuration for maximizing the F1-score.

```{python}
param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 0.9, 1.0]
}

gb_base = GradientBoostingClassifier(random_state=42)

grid_search = GridSearchCV(estimator=gb_base, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=1, scoring='f1')
```

Running the grid search to train and evaluate the model for each combination of hyperparameters. Once complete, the best parameters and their corresponding cross-validation F1-score are displayed.

```{python}
grid_search.fit(X_train_scaled, y_train)

print(f"Best Parameters found: {grid_search.best_params_}")
print(f"Best cross-validation F1-score: {grid_search.best_score_:.4f}")
```

Using the best model from the grid search to make predictions on the training and testing datasets. The tuned model's performance is evaluated using metrics like accuracy, recall, and precision, and a classification report is generated for the test set.

```{python}
best_gb_model = grid_search.best_estimator_

y_pred_test_tuned = best_gb_model.predict(X_test_scaled)
y_pred_train_tuned = best_gb_model.predict(X_train_scaled)

train_accuracy_tuned = accuracy_score(y_train, y_pred_train_tuned)
test_accuracy_tuned = accuracy_score(y_test, y_pred_test_tuned)
recall_tuned = recall_score(y_test, y_pred_test_tuned)
precision_tuned = precision_score(y_test, y_pred_test_tuned)

print(f"Tuned Training Accuracy: {train_accuracy_tuned:.4f}")
print(f"Tuned Test Accuracy: {test_accuracy_tuned:.4f}")
print(f"Tuned Recall: {recall_tuned:.4f}")
print(f"Tuned Precision: {precision_tuned:.4f}")

print("\nClassification Report (Tuned Test Set):")
print(classification_report(y_test, y_pred_test_tuned, target_names=['No Disease (0)', 'Disease (1)']))

```

Here is a new confusion matrix. This should highlight the improved performance at correctly predicting true positives and true negatives. 

```{python}
cm_tuned = confusion_matrix(y_test, y_pred_test_tuned)
print(cm_tuned)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', xticklabels=['No Disease (0)', 'Disease (1)'], yticklabels=['No Disease (0)', 'Disease (1)'])
plt.title('Confusion Matrix (Tuned Test Set)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()
```

### Regular vs. Hyperparameter Tuning

Analyzing the importance of each feature in the tuned model. A bar chart is generated to visualize which features contribute the most to the model's predictions. The top 3 features are highlighted with their relative importance percentages.

```{python}
feature_importances_tuned = best_gb_model.feature_importances_
importance_df_tuned = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances_tuned})
importance_df_tuned = importance_df_tuned.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df_tuned)
plt.title('Tuned Gradient Boosting Feature Importance')
plt.show()

importance_df_tuned['Relative Importance'] = importance_df_tuned['Importance'] / importance_df_tuned['Importance'].sum()
top_3_features_tuned = importance_df_tuned.head(3)
top_3_features_tuned['Relative Importance'] = (top_3_features_tuned['Relative Importance'] * 100).round(0).astype(int)
print("\nTop 3 Features (Tuned Model):\n", top_3_features_tuned[['Feature', 'Relative Importance']])
```

```{python}
feature_importances = gb_model.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

importance_df['Relative Importance'] = importance_df['Importance'] / importance_df['Importance'].sum()

top_3_features = importance_df.head(3)

top_3_features['Relative Importance'] = (top_3_features['Relative Importance'] * 100).round(0).astype(int)

top_3_features[['Feature', 'Relative Importance']]
```

```{python}
print("\nTop 5 Features:")
importance_df.head()
```

## Conclusion

This project successfully developed and evaluated a machine learning model to predict the presence of heart disease using the widely recognized UC Irvine Machine Learning Repository dataset. The primary objective was to build a reliable predictive tool that could potentially aid healthcare professionals in risk stratification.


### Summary of Process and Findings:

1. **Data Preparation:** The analysis began with thorough data exploration, identifying data types, distributions, and potential issues. Key preprocessing steps included:
      - Handling missing values, specifically imputing the thal (Thalassemia test result) column using the mean, ensuring data completeness. The ca (number of major vessels colored by flouroscopy) column also initially had missing values, which were implicitly handled likely during outlier removal or model fitting, though the thal imputation was explicitly addressed.
      - Transforming the multi-class target variable (num, ranging 0-4) into a binary target (disease_binary, 0 for no presence, 1 for any presence), simplifying the task to binary classification.
      - Addressing data outliers using the Interquartile Range (IQR) method for key numerical features like trestbps, chol, thalach, and oldpeak to improve model robustness.
      - Standardizing numerical features using StandardScaler to ensure features were on a comparable scale, which benefits algorithms like Gradient Boosting.

2. **Exploratory Data Analysis:** Visualizations revealed important characteristics:
      - The dataset showed a relatively balanced distribution between patients with and without heart disease (approx. 46% vs. 54%), suitable for building a classification model.
      - Notable trends included a higher prevalence of heart disease in males within this dataset and interesting patterns related to chest pain types, where asymptomatic pain (Type 4) was paradoxically a strong indicator for disease presence.

3. **Model Development and Evaluation:**
      - A Gradient Boosting Classifier was selected for modeling. The data was split into 80% training and 20% testing sets using stratification to maintain class balance.
      - An initial model achieved high training accuracy (0.9959), suggesting potential overfitting, but demonstrated strong performance on the test set with an accuracy of 0.8689, precision of 0.8125, and recall of 0.9286 for the positive (disease) class.
      - Hyperparameter tuning using GridSearchCV with 5-fold cross-validation was performed to optimize the model, targeting the F1-score. The best parameters found were largely similar to the initial ones (learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9).
      - The tuned model yielded a slightly lower test accuracy of 0.8525, but maintained strong recall (0.9286) and slightly lower precision (0.7879) for detecting heart disease. The confusion matrix showed 26 true positives, 26 true negatives, 7 false positives, and 2 false negatives. The emphasis on recall is often important in medical diagnoses to minimize false negatives (missing actual cases).

4. **Feature Importance:** The tuned Gradient Boosting model highlighted the most influential features for prediction:
      - `thal` (Thalassemia result) emerged as the most significant predictor (approx. 29% relative importance).
      - `ca` (Number of major vessels) and cp (Chest pain type) were also highly important (both around 13%).
      - Other contributing factors included age and oldpeak (ST depression induced by exercise).

### Significance and Implications:

The final tuned Gradient Boosting model demonstrated good predictive capability, particularly in identifying patients with heart disease (high recall). With a test accuracy of ~85% and an F1-score of 0.85 for both classes, the model shows promise as a potential decision support tool. The identification of thal, ca, and cp as key predictors aligns with clinical knowledge, reinforcing the model's validity.


### Limitations and Future Work:

- **Dataset Size:** The analysis was based on a relatively small dataset (303 entries before outlier removal), which might limit the generalizability of the findings.
Imputation Method: Mean imputation for thal is a basic approach; more sophisticated methods could be explored.

- **Tuning Impact:** Hyperparameter tuning did not significantly improve test set accuracy over the initial well-performing model in this instance, although it optimized for cross-validated F1-score.
Generalizability: The model should be validated on independent, potentially larger, and more diverse datasets before any clinical application.

Future work could involve exploring other machine learning algorithms (e.g., Random Forests, SVMs, Neural Networks), investigating more advanced feature engineering techniques, employing different strategies for handling missing data and outliers, and performing external validation. Further analysis into the specific misclassifications made by the model could also provide valuable insights for improvement.

In conclusion, this analysis successfully built a Gradient Boosting model capable of predicting heart disease with reasonable accuracy and high recall, identifying key clinical indicators from the data. While further validation and refinement are necessary, the results represent a solid foundation for developing a machine learning-based tool for heart disease risk assessment.